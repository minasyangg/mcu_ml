{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db1b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/.virtualenvs/jupyter/lecture4/python')\n",
    "import needle as ndl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7068effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "from scipy.special import softmax\n",
    "import numdifftools as nd\n",
    "\n",
    "def softmax_loss(Z, y):\n",
    "  log_sum_exp = np.log(np.sum(np.exp(Z), axis=1))\n",
    "  if y.ndim == 1:\n",
    "    for i in range(y.shape[0]):\n",
    "      log_sum_exp[i] -= Z[i, y[i]]\n",
    "    l = np.mean(log_sum_exp)\n",
    "    return l\n",
    "  elif y.ndim == 2:\n",
    "    for i in range(y.shape[0]):\n",
    "      idx = int(np.argwhere(y[i] == 1))\n",
    "      # print(y[i][idx])\n",
    "      log_sum_exp[i] -= Z[i, idx]\n",
    "    l = np.mean(log_sum_exp)\n",
    "    return l\n",
    "\n",
    "\n",
    "def softmax_regression_epoch(X, y, theta, lr = 0.1, batch=50):\n",
    "    for i in range(0, y.shape[0], batch):\n",
    "      x = X[i:i+batch]\n",
    "      y_b = y[i:i+batch]\n",
    "      Z = np.transpose(np.exp(np.dot(x, theta)).T/(np.sum(np.exp(np.dot(x, theta)), axes=1)))      \n",
    "      I = np.zeros((Z.shape[0], theta.shape[1]))      \n",
    "      I[range(batch), y_b] = 1\n",
    "      theta -= (lr/batch)*(np.dot(x.T, (Z-I)))   \n",
    "    return theta\n",
    "\n",
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "  magic_num, items = struct.unpack_from('>ii', f.read(8))\n",
    "  rows, columns = struct.unpack('>ii', f.read(8))\n",
    "  byte_stream = f.read()\n",
    "  images = np.frombuffer(byte_stream, dtype='uint8')\n",
    "  images = images.reshape((items, rows*columns)).astype(np.float32)\n",
    "  images = images/np.max(images)\n",
    "\n",
    "\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "  magic_num, items = struct.unpack_from('>ii', f.read(8))\n",
    "  byte_stream = f.read()\n",
    "  labels = np.frombuffer(byte_stream, dtype='uint8')\n",
    "  labels = labels.reshape(items).astype(dtype=np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def nn_epoch(X, y, W1, W2, lr = 0.1, batch=100):\n",
    "  for i in range(0, y.shape[0], batch):\n",
    "    x = X[i:i+batch]\n",
    "    y_b = y[i:i+batch]\n",
    "    I = np.zeros((x.shape[0],W2.shape[1]))\n",
    "    I[range(batch), y_b] = 1\n",
    "    \n",
    "    \n",
    "    Z1 = np.maximum(0, np.dot(x, W1))\n",
    "    G2 = softmax(np.dot(Z1, W2), axis=1) - I\n",
    "    Z1_b = np.where(Z1>0, 1, 0)\n",
    "    G1 = np.multiply(Z1_b, np.dot(G2, W2.T))\n",
    "\n",
    "    W1 -= (lr/batch)*np.dot(x.T, G1)\n",
    "    W2 -= (lr/batch)*np.dot(Z1.T, G2)\n",
    "  return W1, W2\n",
    "\n",
    "def softmax_regression_ndl(X, y, theta, lr = 0.1, batch=100):\n",
    "\n",
    "  for i in range(0, y.shape[0], batch):\n",
    "    x = X[i:i+batch]\n",
    "    y_b = y[i:i+batch]\n",
    "    Z = np.transpose(np.exp(np.dot(x, theta)).T/(np.sum(np.exp(np.dot(x, theta)), axis=1)))\n",
    "    I = np.zeros((Z.shape[0], theta.shape[1]))\n",
    "    I[range(batch), y_b] = 1\n",
    "    theta -= (lr/batch)*(np.dot(x.T, (Z-I)))\n",
    "\n",
    "  return theta\n",
    "\n",
    "def nn_epoch(X, y, W1, W2, lr = 1, batch=50):\n",
    "  for i in range(0, y.shape[0], batch):\n",
    "    x = X[i:i+batch]\n",
    "    y_b = y[i:i+batch]\n",
    "    I = np.zeros((x.shape[0],W2.shape[1]))\n",
    "    I[range(batch), y_b] = 1    \n",
    "    # Z1 = np.maximum(0, )   \n",
    "    Z1 = np.where(np.dot(x, W1)>0, 1, 0)  \n",
    "    G2 = softmax(np.matmul(Z1, W2), axis=1) - I\n",
    "       \n",
    "    G1 = np.multiply(Z1, np.dot(G2, W2.T))\n",
    "    # print(np.dot(x.T, G1))\n",
    "    W1 -= (lr/batch)*np.dot(x.T, G1)\n",
    "    W2 -= (lr/batch)*np.dot(Z1.T, G2)\n",
    "    # print(W2)\n",
    "  return W1, W2\n",
    "\n",
    "def softmax_loss_ndl(Z, y_one_hot):\n",
    "  log_sum_exp = ndl.log(ndl.summation(ndl.exp(Z), axes=1)).reshape((-1, 1))\n",
    "  # print(log_sum_exp.shape)\n",
    "  # print(Z.shape)\n",
    "  # print(y_one_hot.shape)\n",
    "  log_sum_exp = log_sum_exp + ndl.negate(ndl.multiply(Z, y_one_hot))\n",
    "  l = ndl.divide_scalar(ndl.summation(log_sum_exp), log_sum_exp.cached_data.size)\n",
    "  return l\n",
    "\n",
    "def nn_epoch_ndl(X, y, W1, W2, lr = 1, batch=50):\n",
    "  \n",
    "  for i in range(0, y.shape[0], batch):\n",
    "    x = ndl.Tensor(X[i:i+batch])\n",
    "    y_b = y[i:i+batch]\n",
    "    I = ndl.Tensor(np.zeros((x.shape[0],W2.shape[1])))\n",
    "    I.cached_data[range(batch), y_b] = 1\n",
    "    Z1 = ndl.relu(ndl.matmul(x, W1))\n",
    "    logits = ndl.divide(ndl.exp(ndl.matmul(Z1, W2)), ndl.summation(ndl.exp(ndl.matmul(Z1, W2)), axes=1))\n",
    "    loss = softmax_loss_ndl(logits, I)\n",
    "    loss.backward()\n",
    "    W1 -= (lr)*W1.grad\n",
    "    W2 -= (lr)*W2.grad\n",
    "    \n",
    "  return W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280a5aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.36733624  0.11524972 -0.0044524 ]\n",
      " [ 0.6786693   0.724955   -0.16219284]\n",
      " [-0.21184002  0.9547785  -0.04942678]\n",
      " [ 0.16936992  0.8586685  -0.36862653]\n",
      " [ 0.40920788 -0.0476474  -0.39686117]\n",
      " [-0.8148212   0.35875607  0.17811905]\n",
      " [-0.19106793 -0.28013968 -0.03476331]\n",
      " [-0.9004626  -0.16824295  0.46423113]\n",
      " [ 0.42482972 -0.11699399  0.16757399]\n",
      " [-0.4935292  -0.1882925  -0.54978293]]\n",
      "0\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/root/.virtualenvs/jupyter/lecture4/python')\n",
    "import needle as ndl\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(50,5).astype(np.float32)\n",
    "y = np.random.randint(3, size=(50,)).astype(np.uint8)\n",
    "W1 = np.random.randn(5, 10).astype(np.float32) / np.sqrt(10)\n",
    "W2 = np.random.randn(10, 3).astype(np.float32) / np.sqrt(3)\n",
    "print(W2)\n",
    "\n",
    "\n",
    "W1_0, W2_0 = W1.copy(), W2.copy()\n",
    "W1 = ndl.Tensor(W1)\n",
    "W2 = ndl.Tensor(W2)\n",
    "X_ = ndl.Tensor(X)\n",
    "y_one_hot = np.zeros((y.shape[0], 3))\n",
    "y_one_hot[np.arange(y.size), y] = 1\n",
    "y_ = ndl.Tensor(y_one_hot)\n",
    "\n",
    "\n",
    "W1, W2 = nn_epoch_ndl(X, y, W1, W2, lr=1.0, batch=50)\n",
    "# W1\n",
    "# print(W1, '\\n\\n', W2)\n",
    "dW1 = nd.Gradient(lambda W1_ : softmax_loss_ndl(ndl.relu(X_@ndl.Tensor(W1_).reshape((5,10))@W2), y_).numpy())(W1.numpy())\n",
    "\n",
    "dW2 = nd.Gradient(lambda W2_ : softmax_loss_ndl(ndl.relu(X_@W1)@ndl.Tensor(W2_).reshape((10,3)), y_).numpy())(W2.numpy())\n",
    "# print(W1, '\\n',W2)\n",
    "# print(W1_0-W1.numpy())\n",
    "# print(dW1.reshape(5, 10))\n",
    "# # print('\\n')\n",
    "# print(W2_0-W2.numpy())\n",
    "# print(dW2.reshape((10, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade89d4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type Tensor which has no callable exp method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'exp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m dW1 \u001b[38;5;241m=\u001b[39m nd\u001b[38;5;241m.\u001b[39mGradient(\u001b[38;5;28;01mlambda\u001b[39;00m W1_ : softmax_loss_ndl(ndl\u001b[38;5;241m.\u001b[39mrelu(X_\u001b[38;5;129m@ndl\u001b[39m\u001b[38;5;241m.\u001b[39mTensor(W1_)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m)))\u001b[38;5;129m@W2\u001b[39m, y_)\u001b[38;5;241m.\u001b[39mnumpy())(W1\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     10\u001b[0m dW1\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m dW2 \u001b[38;5;241m=\u001b[39m \u001b[43mnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGradient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mW2_\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftmax_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mndl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[38;5;129;43m@W1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;129;43m@ndl\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW2_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/jupyter/lib/python3.10/site-packages/numdifftools/core.py:490\u001b[0m, in \u001b[0;36mGradient.__call__\u001b[0;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m--> 490\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(Gradient, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(np\u001b[39m.\u001b[39;49matleast_1d(x)\u001b[39m.\u001b[39;49mravel(), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    491\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_output:\n\u001b[1;32m    492\u001b[0m         \u001b[39mreturn\u001b[39;00m result[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msqueeze(), result[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.virtualenvs/jupyter/lib/python3.10/site-packages/numdifftools/core.py:431\u001b[0m, in \u001b[0;36mJacobian.__call__\u001b[0;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m--> 431\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(Jacobian, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(np\u001b[39m.\u001b[39;49matleast_1d(x), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/.virtualenvs/jupyter/lib/python3.10/site-packages/numdifftools/core.py:288\u001b[0m, in \u001b[0;36mDerivative.__call__\u001b[0;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[1;32m    286\u001b[0m x_i \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\n\u001b[1;32m    287\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(divide\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m, invalid\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 288\u001b[0m     results, f_xi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_derivative(x_i, args, kwds)\n\u001b[1;32m    289\u001b[0m     derivative, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extrapolate(\u001b[39m*\u001b[39mresults)\n\u001b[1;32m    290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_output:\n",
      "File \u001b[0;32m~/.virtualenvs/jupyter/lib/python3.10/site-packages/numdifftools/core.py:422\u001b[0m, in \u001b[0;36mJacobian._derivative_nonzero_order\u001b[0;34m(self, x_i, args, kwds)\u001b[0m\n\u001b[1;32m    420\u001b[0m diff, f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_functions(args, kwds)\n\u001b[1;32m    421\u001b[0m steps, step_ratio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_steps(x_i)\n\u001b[0;32m--> 422\u001b[0m fxi \u001b[39m=\u001b[39m f(x_i)\n\u001b[1;32m    423\u001b[0m results \u001b[39m=\u001b[39m [diff(f, fxi, x_i, h) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m steps]\n\u001b[1;32m    425\u001b[0m steps2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_steps(steps, x_i, fxi)\n",
      "File \u001b[0;32m~/.virtualenvs/jupyter/lib/python3.10/site-packages/numdifftools/core.py:257\u001b[0m, in \u001b[0;36mDerivative._get_functions.<locals>.export_fun\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport_fun\u001b[39m(x):\n\u001b[0;32m--> 257\u001b[0m     \u001b[39mreturn\u001b[39;00m fun(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "Cell \u001b[0;32mIn [4], line 11\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(W2_)\u001b[0m\n\u001b[1;32m      9\u001b[0m dW1 \u001b[38;5;241m=\u001b[39m nd\u001b[38;5;241m.\u001b[39mGradient(\u001b[38;5;28;01mlambda\u001b[39;00m W1_ : softmax_loss_ndl(ndl\u001b[38;5;241m.\u001b[39mrelu(X_\u001b[38;5;129m@ndl\u001b[39m\u001b[38;5;241m.\u001b[39mTensor(W1_)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m)))\u001b[38;5;129m@W2\u001b[39m, y_)\u001b[38;5;241m.\u001b[39mnumpy())(W1\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     10\u001b[0m dW1\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m dW2 \u001b[38;5;241m=\u001b[39m nd\u001b[38;5;241m.\u001b[39mGradient(\u001b[38;5;28;01mlambda\u001b[39;00m W2_ : \u001b[43msoftmax_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mndl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[38;5;129;43m@W1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;129;43m@ndl\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW2_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())(W2\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[0;32mIn [2], line 8\u001b[0m, in \u001b[0;36msoftmax_loss\u001b[0;34m(Z, y)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax_loss\u001b[39m(Z, y):\n\u001b[0;32m----> 8\u001b[0m   log_sum_exp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(np\u001b[38;5;241m.\u001b[39msum(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n",
      "\u001b[0;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type Tensor which has no callable exp method"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.randn(50,5).astype(np.float32)\n",
    "y = np.random.randint(3, size=(50,)).astype(np.uint8)\n",
    "W1 = np.random.randn(5, 10).astype(np.float32) / np.sqrt(10)\n",
    "W2 = np.random.randn(10, 3).astype(np.float32) / np.sqrt(3)\n",
    "W1_0, W2_0 = W1.copy(), W2.copy()\n",
    "W1 = ndl.Tensor(W1)\n",
    "W2 = ndl.Tensor(W2)\n",
    "dW1 = nd.Gradient(lambda W1_ : softmax_loss_ndl(ndl.relu(X_@ndl.Tensor(W1_).reshape((5,10)))@W2, y_).numpy())(W1.numpy())\n",
    "dW1.reshape(5, 10)\n",
    "dW2 = nd.Gradient(lambda W2_ : softmax_loss(ndl.relu(X_@W1)@ndl.Tensor(W2_).reshape((10,3)), y_).numpy())(W2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11632aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ndl.Tensor(np.random.randn(6, 4))\n",
    "b = ndl.Tensor(np.random.randn(4, 5))\n",
    "c = ndl.Tensor(np.random.randn(5, 5))\n",
    "\n",
    " \n",
    "\n",
    "y = a@b + c\n",
    "y.backward()\n",
    "c.grad\n",
    "\n",
    "y.backward()\n",
    "c.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6ec58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "741a2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/.virtualenvs/jupyter/lecture4/python')\n",
    "import needle as ndl\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "from scipy.special import softmax\n",
    "import numdifftools as nd\n",
    "\n",
    "def softmax_loss_ndl(Z, y_one_hot):\n",
    "  log_sum_exp = ndl.log(ndl.summation(ndl.exp(Z), axes=1)).reshape((-1, 1))\n",
    "  log_sum_exp -=  Z*y_one_hot\n",
    "  l = ndl.divide_scalar(ndl.summation(log_sum_exp), log_sum_exp.cached_data.size)\n",
    "  return l\n",
    "\n",
    "def test_nn_epoch_ndl(X, W1, W2):\n",
    "  x = ndl.Tensor(X)\n",
    "  # y_b = y\n",
    "  # I = ndl.Tensor(np.zeros((x.shape[0],W2.shape[1])))\n",
    "  # I.cached_data[range(50), y_b.numpy()] = 1\n",
    "  Z1 = ndl.relu(ndl.matmul(X, W1))\n",
    "  logits = ndl.matmul(Z1, W2)\n",
    "\n",
    "  return W2-W2.grad\n",
    "\n",
    "\n",
    "\n",
    "def gradient_check(f, *args, tol=1e-6, backward=False, **kwargs):\n",
    "    eps = 1e-4\n",
    "    numerical_grads = [np.zeros(a.shape) for a in args]\n",
    "    for i in range(len(args)):\n",
    "        for j in range(args[i].realize_cached_data().size):\n",
    "            args[i].realize_cached_data().flat[j] += eps\n",
    "            f1 = float(f(*args, **kwargs).numpy().sum())\n",
    "            args[i].realize_cached_data().flat[j] -= 2 * eps\n",
    "\n",
    "            f2 = float(f(*args, **kwargs).numpy().sum())\n",
    "            args[i].realize_cached_data().flat[j] += eps\n",
    "            numerical_grads[i].flat[j] = (f1 - f2) / (2 * eps)\n",
    "    if not backward:\n",
    "        out = f(*args, **kwargs)\n",
    "        computed_grads = [x.numpy() for x in out.op.gradient_as_tuple(ndl.Tensor(np.ones(out.shape)), out)]\n",
    "        # print(f'my val:{computed_grads[0]}')\n",
    "        # print(f'true_val{numerical_grads[0]}')\n",
    "    else:\n",
    "        out = f(*args, **kwargs).sum()\n",
    "        # print(out)\n",
    "        out.backward()\n",
    "        # args[0].grad\n",
    "        # print(numerical_grads[1].shape)\n",
    "        computed_grads = [a.grad.numpy() for a in args]\n",
    "        # computed_grads = [args[0].grad.numpy()]\n",
    "        print(f'my val:{computed_grads[2]}')\n",
    "        # print(f'true_val{numerical_grads[2]}')\n",
    "    error = sum(\n",
    "        np.linalg.norm(computed_grads[i] - numerical_grads[i])\n",
    "        for i in range(len(args))\n",
    "    )\n",
    "\n",
    "    # assert error < tol\n",
    "    return bool(error < tol)\n",
    "\n",
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "  magic_num, items = struct.unpack_from('>ii', f.read(8))\n",
    "  rows, columns = struct.unpack('>ii', f.read(8))\n",
    "  byte_stream = f.read()\n",
    "  images = np.frombuffer(byte_stream, dtype='uint8')\n",
    "  images = images.reshape((items, rows*columns)).astype(np.float32)\n",
    "  images = images/np.max(images)\n",
    "\n",
    "\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "  magic_num, items = struct.unpack_from('>ii', f.read(8))\n",
    "  byte_stream = f.read()\n",
    "  labels = np.frombuffer(byte_stream, dtype='uint8')\n",
    "  labels = labels.reshape(items).astype(dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aa81312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my val:[[28. 28. 28.]\n",
      " [28. 28. 28.]\n",
      " [25. 25. 25.]\n",
      " [25. 25. 25.]\n",
      " [30. 30. 30.]\n",
      " [27. 27. 27.]\n",
      " [26. 26. 26.]\n",
      " [26. 26. 26.]\n",
      " [24. 24. 24.]\n",
      " [23. 23. 23.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        ...,\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        ...,\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        ...,\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.randn(50,5).astype(np.float32)\n",
    "y = np.random.randint(3, size=(50,)).astype(np.uint8)\n",
    "W1 = np.random.randn(5, 10).astype(np.float32) / np.sqrt(10)\n",
    "W2 = np.random.randn(10, 3).astype(np.float32) / np.sqrt(3)\n",
    "\n",
    "\n",
    "W1_0, W2_0 = W1.copy(), W2.copy()\n",
    "W1 = ndl.Tensor(W1)\n",
    "W2 = ndl.Tensor(W2)\n",
    "X_ = ndl.Tensor(X)\n",
    "y_one_hot = np.zeros((y.shape[0], 3))\n",
    "y_one_hot[np.arange(y.size), y] = 1\n",
    "y_ = ndl.Tensor(y_one_hot)\n",
    "\n",
    "\n",
    "gradient_check(test_nn_epoch_ndl, ndl.Tensor(X), W1, W2, tol=0.01, backward=True)\n",
    "\n",
    "dW2 = nd.Gradient(lambda W2_: test_nn_epoch_ndl(ndl.Tensor(X), W1, ndl.Tensor(W2_).reshape((10, 3))).numpy())(W2.numpy())\n",
    "dW2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2857242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<needle.ops.Summation object at 0x7efe73c81420>\n",
      "<needle.ops.DivScalar object at 0x7efe73c83310>\n",
      "<needle.ops.Summation object at 0x7efe73c81510>\n",
      "<needle.ops.EWiseAdd object at 0x7efe73c828c0>\n",
      "<needle.ops.Negate object at 0x7efe73c82bc0>\n",
      "<needle.ops.EWiseMul object at 0x7efe73c832b0>\n",
      "None\n",
      "<needle.ops.Reshape object at 0x7efe73c80bb0>\n",
      "<needle.ops.Log object at 0x7efe73c81150>\n",
      "<needle.ops.Summation object at 0x7efe73c812a0>\n",
      "<needle.ops.Exp object at 0x7efe74968670>\n",
      "None\n",
      "my val:[[ 0.01136483  0.00290557  0.00518209  0.01830847  0.01260418  0.00073286\n",
      "   0.00503573 -0.00457617  0.00175637  0.00293606]\n",
      " [ 0.00385512  0.01429103  0.00089488  0.00376984  0.00520293  0.00466008\n",
      "   0.01487137  0.00271883  0.00456504  0.00142086]\n",
      " [ 0.00018995 -0.00155904  0.00579186  0.00116166  0.02361241  0.00056987\n",
      "   0.0025543   0.00202351  0.01129994  0.01060554]\n",
      " [-0.00073012  0.00690035  0.0019457   0.00065221  0.00333842  0.00552762\n",
      "   0.01617877  0.01573345  0.00320939  0.00349421]\n",
      " [ 0.001749    0.00120633  0.00090604  0.0351061  -0.00325199  0.00322046\n",
      "   0.0014259   0.01086004  0.00099372  0.00403441]\n",
      " [ 0.00285435  0.00404041  0.00419337  0.00214615  0.0067946   0.01072569\n",
      "   0.00746949  0.00945725  0.00370613  0.00486258]\n",
      " [ 0.0046007   0.00629097  0.00399692  0.00160382  0.0045128   0.00603085\n",
      "   0.00176556  0.01431705  0.00363777  0.00949357]\n",
      " [ 0.01137079  0.00623975  0.01713902  0.00159541  0.00820135  0.00276529\n",
      "   0.00229598  0.00307439  0.00401647 -0.00044847]\n",
      " [ 0.00084865  0.00669841  0.00433493  0.00058555  0.01205279  0.01186849\n",
      "   0.00884475  0.00227304  0.00093267  0.00781071]\n",
      " [ 0.0019876   0.01010014  0.00366335  0.00789906  0.00424809  0.00602958\n",
      "   0.00300598  0.01774271  0.0033771  -0.00180361]\n",
      " [ 0.00771835  0.00055205  0.0005964   0.00560153  0.00065739  0.01483909\n",
      "   0.00140498  0.00100621  0.01453538  0.00933862]\n",
      " [ 0.01502885  0.00574573  0.00098136  0.01568143  0.00177608  0.00518033\n",
      "  -0.00026256  0.00198854  0.00429088  0.00583935]\n",
      " [ 0.00486224  0.00111146  0.00449656  0.01257184  0.00166613  0.00287324\n",
      "   0.00215959  0.02120714  0.00653628 -0.00123449]\n",
      " [-0.00352717  0.01008284  0.00299593  0.00607035  0.00311348  0.01156542\n",
      "   0.01046647  0.00477449  0.00873723  0.00197097]\n",
      " [ 0.00062217 -0.00196077  0.00326542  0.00521608  0.0299599   0.00710781\n",
      "   0.00110948  0.00844632  0.00074142  0.00174217]\n",
      " [ 0.0044215   0.02626044  0.00224783  0.00207152  0.00428992 -0.00381184\n",
      "   0.0146049   0.00160768  0.00150268  0.00305537]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X, y = images, labels\n",
    "Zsmall = ndl.Tensor(np.random.randn(16, 10).astype(np.float32))\n",
    "Z = ndl.Tensor(np.zeros((y.shape[0], 10)).astype(np.float32))\n",
    "y_one_hot = np.zeros((y.shape[0], 10))\n",
    "y_one_hot[np.arange(y.size), y] = 1\n",
    "ysmall = ndl.Tensor(y_one_hot[:16])\n",
    "y = ndl.Tensor(y_one_hot)\n",
    "Z = ndl.Tensor(np.random.randn(y.shape[0], 10).astype(np.float32))\n",
    "# softmax_loss_ndl(Z,y)\n",
    "\n",
    "gradient_check(softmax_loss_ndl, Zsmall, ysmall, tol=0.001, backward=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312605d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_check(ndl.relu, ndl.Tensor(np.random.randn(5,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285b3a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [160], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m idx1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margwhere(a\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m idx1\n\u001b[0;32m----> 9\u001b[0m \u001b[43ma\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx1\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "from numpy import broadcast_to\n",
    "\n",
    "\n",
    "a = np.array([[1,2],[3,4]])\n",
    "\n",
    "idx1 = np.argwhere(a>2)\n",
    "idx1\n",
    "\n",
    "a(idx1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/root/.virtualenvs/jupyter/lecture4/python')\n",
    "import needle as ndl\n",
    "a = ndl.Tensor(np.array(np.random.randn(50, 1)))\n",
    "b = ndl.Tensor(np.array(np.random.randn(50, 1)))\n",
    "\n",
    "# b.reshape((50,1,1, 1))\n",
    "c = np.divide(a, b)\n",
    "c.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('jupyter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1624f513274f71c039b0eafa82d195dcf9ffc19b674747cc028e5fa1cb23be33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
